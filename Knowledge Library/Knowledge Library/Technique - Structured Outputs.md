# Practical Guide to Structured Outputs and Tool Calling for LLMs

**Author:** Manus AI  
**Date:** November 20, 2025

> **Note:** This document was generated by Manus AI for research purposes and should not be considered investment advice.

## Introduction

Large Language Models (LLMs) have evolved from simple text generators into reasoning engines that can interact with external systems, execute actions, and automate complex workflows. At the heart of this transformation is the ability to produce **structured outputs** and reliably use **tool calling**. Moving from free-form text responses to predictable, machine-readable formats like JSON is essential for integrating LLMs into production applications. However, ensuring this structured communication is consistent, valid, and resilient to errors presents significant technical challenges.

This guide covers the techniques and patterns for structured outputs and tool orchestration in LLMs. We'll explore native implementations from major providers (OpenAI, Anthropic, and Google), discuss the evolution from function calling to parallel and free-form executions, and detail validation and recovery strategies for handling malformed outputs. We'll also examine agent orchestration patterns, schema versioning best practices, and the latest API updates like GPT-5's Custom Tools and Claude's Structured Outputs.

**Key Concepts:**
- **Structured Outputs:** The practice of forcing an LLM to generate responses in a specific, well-defined data format like JSON, essential for interoperability between the LLM and other system components.
- **Function/Tool Calling:** An advancement of structured outputs where the LLM generates JSON that corresponds to a function or tool signature that the system can execute. The model understands the function's semantics and generates correct arguments for the call.
- **Automatic Evaluation:** Structured outputs form the foundation for automatic LLM evaluation. By forcing output into a predictable format, it's possible to write unit and integration tests to validate accuracy, compliance, and quality of model responses at scale, without requiring manual human evaluation for each response.
- **LLM-as-Judge:** An evaluation technique where a second LLM is used to judge the quality of the first LLM's output, comparing the generated structured output with a reference answer (ground truth).

## 1. The Evolution of Structured Outputs: From JSON Mode to JSON Schema

The need for predictable data formats drives the adoption of structured outputs. Production applications can't rely on parsing free-form text, which is inherently fragile and prone to breaking when models are updated or edge cases arise. The evolution can be seen in two major stages: **JSON Mode** and **JSON Schema Mode**.

### 1.1. JSON Mode: Syntax Guarantee

**JSON Mode** was the first major solution offered by LLM providers to solve malformed output problems. When enabled, the model is forced to generate a string that is always a syntactically valid JSON object. This eliminates the need for prompt engineering hacks or regex fixes to ensure the output can be parsed by a standard JSON parser.

However, JSON Mode only guarantees syntactic validity, not structural or semantic validity. The model can still omit fields, invent new field names, or change data types, leading to failures in downstream systems. For example, a field `"priority": "high"` might become `"urgency": "HIGH"` in a future response, breaking application logic that expects the `priority` field.

### 1.2. JSON Schema: Structure Guarantee

**JSON Schema Mode** (often called Structured Outputs) represents a significant advancement. Instead of just guaranteeing valid JSON, this approach forces the model's output to adhere to a specific JSON schema provided by the developer. This schema acts as a contract, defining expected fields, their data types, whether they're required, and other constraints.

**What is JSON Schema?** JSON Schema is a specification that defines the structure and validation rules for JSON data. It describes what the JSON must contain before it's even created, specifying required fields, data types, value constraints, and nested object structures.

This approach moves the responsibility of ensuring correct structure from the developer (through post-processing parsing and validation) to the model during generation. Tools like Pydantic (for Python) and Zod (for TypeScript) have become standard for generating these schemas from native code classes, creating a robust pipeline from type definition to validated output.

## 2. The Evolution of Tool Calling: Beyond Rigid JSON

Tool calling is a specialized application of structured outputs, where the LLM doesn't just format data but also requests the execution of an external function or tool. The evolution here also reflects a transition from rigid structures to more flexible and powerful interactions, culminating in advanced agentic capabilities.

### 2.1. Traditional Function Calling (JSON Schema)

Initially, function calling operated strictly based on JSON Schema. The developer defined a set of tools, each with a description and a parameter schema. The LLM would then decide which tool to call and generate a JSON object with the corresponding arguments. This approach is highly reliable and predictable, ideal for interactions with APIs where argument types and structure are strict.

### 2.2. GPT-5 and Custom Tools (Free-Form)

With the launch of GPT-5, OpenAI introduced **Custom Tools**, which represent a paradigm shift. These tools allow the model to send raw, unstructured text (free-form) directly to a tool, without needing a JSON Schema.

**Custom Tools Characteristics:**
- **Flexibility:** Ideal for runtimes that expect raw text, like SQL scripts, shell commands, or configurations in domain-specific languages (DSLs).
- **Rapid Iteration:** Enables faster development and testing without the overhead of defining and maintaining schemas.
- **Agentic Power:** The ability to generate code or commands directly enables more complex and direct agentic workflows.

This approach coexists with traditional Function Tools, allowing developers to choose the right tool for the job: strict validation with JSON Schema when predictability is critical, and free-form text flexibility when integrating with text-based systems.

### 2.3. Grammar Constraints (Lark/CFG)

Another powerful innovation introduced with GPT-5 is support for **Context-Free Grammars (CFG)**, using the Lark format. This functionality allows a tool's output to be strictly controlled by a formal grammar.

**What is Constrained Decoding?** Constrained decoding with CFG ensures that the model's output always follows a predefined syntactic structure, like that of a programming language, a DSL, or a mathematical expression.

This is extremely valuable for high-criticality use cases, such as generating SQL queries, creating infrastructure configurations, or formulating arithmetic expressions, where a single syntax error can cause catastrophic failures. The grammar eliminates the need for post-generation syntax validation, ensuring the output is always valid by construction.

## 3. Provider Comparison: OpenAI, Anthropic, and Google

The three major LLM providers—OpenAI, Anthropic, and Google—approach structured outputs with nuanced differences in their API implementations. While the goal is the same (reliable, predictable outputs), the methods and guarantees vary.

### 3.1. OpenAI (GPT-4, GPT-5)

- **JSON Mode:** OpenAI offers native `json_mode` that guarantees a syntactically valid JSON output. It's activated by setting `response_format={ "type": "json_object" }`.
- **Structured Outputs:** OpenAI's approach to schema adherence is deeply integrated with the Python ecosystem, particularly with the Pydantic library. When using function calling mode (`function_call`), developers can pass a JSON Schema (usually derived from a Pydantic class) and force the model to return arguments that match that schema.
- **Custom Tools & Grammar:** With GPT-5, OpenAI expanded capabilities to include `custom_tools` (free-form text) and `grammar` constraints (Lark/CFG), offering the highest flexibility among providers.

### 3.2. Anthropic (Claude)

- **JSON Mode:** Claude doesn't have a native "JSON mode" that forces JSON output. Getting JSON relies on prompt instructions, which doesn't guarantee syntactic validity.
- **Structured Outputs:** In November 2025, Anthropic launched structured outputs support in public beta for Sonnet 4.5 and Opus 4.1 models. The approach is entirely tool-based. The developer defines a schema as a tool, and Claude guarantees that the output (the tool use) conforms to that schema. This eliminates the need for failover logic and retries for parsing errors.

### 3.3. Google (Gemini)

- **JSON Mode:** Gemini supports native JSON mode, activated by setting `response_mime_type` to `application/json`.
- **Structured Outputs:** Gemini offers native schema enforcement through the Vertex AI API. Developers can specify a strict `response_schema`, and the model guarantees compliance. If the output doesn't match the schema, a `JSONSchemaValidationError` is raised, providing clear error messages for debugging.

**Comparison Table:**

| Feature | OpenAI (GPT-4/5) | Anthropic (Claude) | Google (Gemini) |
| :--- | :--- | :--- | :--- |
| **Native JSON Mode** | Yes (`response_format`) | No (prompt-based only) | Yes (`response_mime_type`) |
| **Schema Guarantee** | Yes (via `function_call`) | Yes (via `tool-use`) | Yes (native with `response_schema`) |
| **Flexibility** | Very High (JSON, Schema, Free-form, Grammar) | Medium (Schema via tools) | High (JSON, Schema) |
| **Error Mechanism** | Validation error in client library | Tool parsing error | Native `JSONSchemaValidationError` |
| **Maturity** | Mature and evolving | Recent (November 2025) | Mature |

## 4. Validation Strategies and Recovery Patterns

Ensuring an LLM's output conforms to a schema is only half the battle. The other half is validating the *content* of that output and having robust strategies for handling failures. Validation has evolved from simple rule checks to sophisticated semantic approaches, and recovery patterns have become more intelligent and automated.

### 4.1. Beyond Rule-Based Validation

Traditional software validation focuses on explicit rules: is the data type correct? Is the value within a predefined range? Does the format match a regular expression? While essential, these checks are insufficient for validating natural language, which is inherently complex and subjective. Criteria like "the description must be professional" or "the critique must be constructive" can't be captured by simple rules.

### 4.2. Semantic Validation with LLMs

**Semantic validation** emerges as a powerful solution to this problem. Instead of explicit rules, this approach uses an LLM to interpret and evaluate content based on criteria expressed in natural language. Libraries like `instructor` for Python have popularized this technique, allowing developers to attach LLM-based validators directly to data model fields (like a Pydantic class).

**How it works:** A semantic validator (`llm_validator`) receives a natural language description of requirements. When the main LLM's output is generated, a second LLM call (usually to a smaller, faster model) is made to evaluate whether the generated content meets those criteria.

This technique is particularly effective for:
- **Content Moderation:** Verifying compliance with community guidelines.
- **Tone and Style Enforcement:** Ensuring communication maintains a consistent brand voice.
- **Fact-Checking:** Evaluating the factual accuracy of a claim.
- **Consistency Validation:** Ensuring different parts of the output (e.g., a summary and its key points) are consistent with each other.

### 4.3. Recovery Patterns: Self-Healing with Retries

Handling validation failures is crucial in production. The most advanced pattern that has emerged is **self-healing** through retries with error context. Instead of simply failing, the system tells the LLM *why* its output was invalid and asks it to correct it.

**Self-Healing Flow:**
1. The LLM generates an output.
2. The output passes through one or more validation layers (types, rules, semantics).
3. If validation fails, the error message (e.g., "The summary doesn't accurately reflect the key points because it omits the main conclusion") is packaged.
4. A new call is made to the LLM, including the original prompt, the incorrect output, and the validation error context.
5. The LLM, now aware of its error, attempts to generate a new output that meets the criteria.

Libraries like `instructor` automate this cycle with a simple `max_retries` parameter. This creates a resilient system that can recover from validation failures without developer intervention, significantly increasing the reliability of the end-to-end pipeline.

### 4.4. Best Practices for Structured Outputs

| Practice | Description | Example Application |
| :--- | :--- | :--- |
| **Use JSON Schema** | Instead of just saying "respond in JSON," provide a detailed JSON Schema. This gives the model an explicit guide about structure, types, and constraints. | Include a `"$schema": "http://json-schema.org/draft-07/schema#"` block and define `properties`, `type`, and `required` for each field. |
| **Clear Instructions in Prompt** | Even with the schema, reinforce in the prompt the instruction to strictly adhere to the format. | `"Your response MUST be a valid JSON object that conforms to the provided schema. Do not add any text before or after the JSON."` |
| **Semantic Descriptions (Function Calling)** | When defining a function, use names and descriptions that clearly communicate what the function does. The LLM uses this semantics to decide when to call it. | For a weather function: `name: "get_weather"`, `description: "Gets the current weather for a specific location."` |
| **Validation and Error Handling** | Always validate the LLM's output against the schema or function signature in your code. Be prepared to handle malformed outputs or incorrect function calls. | Use a JSON validation library. If validation fails, return an error to the LLM and ask it to correct its previous output. |
| **Keep Tools Simple** | Design tools that are atomic and do one thing well. It's better to have several simple tools than one monolithic tool with many operation modes. | Instead of a function `manage_user(action, id, data)`, create `create_user(data)`, `get_user(id)`, `update_user(id, data)`. |
| **Use Tool Output** | After executing a tool, pass the result back to the LLM. This allows it to use the information to generate a coherent final response or decide the next step. | If the `Action` was `calculator(2+2)`, the `Observation` back to the LLM should be `4`. |
| **Schema-Based Evaluation** | For automatic evaluation, compare the generated JSON with a reference JSON (ground truth). Evaluation can verify key presence, type correctness, and value accuracy. | Use a testing framework to perform a "deep equal" between the generated JSON object and the expected one. |
| **Few-Shot Prompting for Function Calling** | Provide examples of conversations that led (or didn't lead) to a function call. This helps the model understand when and how to use tools correctly. | Include example interactions showing correct tool usage patterns in your system prompt or few-shot examples. |

## 5. Advanced Tool Orchestration: From Sequential Chains to Parallel Execution

An LLM's ability to use a single tool is powerful, but the true potential of agentic systems is unlocked through **orchestrating multiple tools** to accomplish complex tasks. The approach to this orchestration has evolved from simple sequential chains to much more efficient and capable parallel and dynamic patterns.

### 5.1. Limitations of Sequential Orchestration

Initial methods like ReAct (Reason and Act) and other chaining patterns operate strictly sequentially. The model executes a tool, observes the result, and based on that, decides the next step. While effective for linear tasks, this approach suffers from several limitations:

- **Limited Perceptual Scope:** The model can only see the result of one tool at a time, which can lead to suboptimal planning.
- **High Latency:** Sequential execution means total time is the sum of each tool call's time, which is unacceptable for many real-time applications.
- **Inefficiency:** Tasks that could be executed in parallel are forced into a queue, wasting time and computational resources.

### 5.2. Orchestration Patterns and the Rise of Parallelism

Providers like Anthropic have identified a set of architectural patterns for agentic systems that go beyond simple chaining. These patterns introduce concepts like routing and parallelism to create more sophisticated workflows.

**Agent Orchestration Patterns (Anthropic):**

| Pattern | Description | When to Use |
| :--- | :--- | :--- |
| **Prompt Chaining** | A sequence of LLM calls, where one's output feeds the next. | Tasks that can be decomposed into fixed, sequential subtasks. |
| **Routing** | An initial LLM classifies input and directs it to a specialized workflow. | Complex tasks with distinct categories that are best handled separately. |
| **Parallelization** | Independent subtasks are executed in parallel and their results are aggregated. | When speed is critical or multiple perspectives are needed for confidence. |
| **Orchestrator-Workers** | A central LLM dynamically decomposes the task and delegates to "worker" LLMs. | Complex tasks where subtasks can't be predicted in advance. |
| **Evaluator-Optimizer** | One LLM generates a response while another evaluates it and provides feedback in an iterative cycle. | When clear evaluation criteria exist and iterative refinement adds value. |

Native support for **parallel function calling** in the latest APIs (like OpenAI's) was a crucial enabler for these patterns, allowing the model to request execution of multiple independent tools in a single step.

### 5.3. The Divide-Then-Aggregate (DTA) Paradigm

While the patterns above provide a conceptual framework, academic research has formalized an even more efficient approach: the **Divide-Then-Aggregate (DTA)** paradigm. Proposed in the paper "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation," DTA-Llama introduces a new way of thinking about tool invocation.

**DTA Innovations:**

1. **From Tree to DAG:** DTA transforms tool search paths, traditionally seen as trees, into a **Directed Acyclic Graph (DAG)**. This better captures real dependencies between tools and exposes parallelism opportunities that a tree hides.
2. **Workflow:** The model learns to **divide** the current task iteratively into subtasks that can be executed in parallel, invokes the tools, and then **aggregates** the results to decide next steps.
3. **Efficiency:** Experiments showed this approach substantially improves task performance while **reducing token consumption and inference time**. Notably, a Llama2-7B model using the DTA method achieved performance comparable to GPT-3.5's native parallel function calling method, a much larger model.

This approach represents the state of the art in tool orchestration, moving from static or sequential planning to dynamic, parallel execution planning, inspired by process and thread mechanisms in operating systems.

## 6. Schema Versioning: Managing Evolution in Production

As LLM-based applications mature, structured output schemas and tool definitions inevitably need to evolve. Managing this evolution without breaking existing integrations is a critical challenge, borrowed directly from API development best practices and database schema migration patterns.

### 6.1. The Schema Evolution Problem

Schema changes can be classified as **breaking** or **non-breaking**.

- **Breaking Changes:**
  - Removing a required field.
  - Changing an existing field's type (e.g., from `string` to `integer`).
  - Adding new validation that makes old data invalid.
- **Non-Breaking Changes (Backward-Compatible):**
  - Adding a new optional field.
  - Adding a new value to an `enum`.
  - Marking an existing field as `deprecated` instead of removing it.

In an LLM ecosystem, where the model can be a "client" of your schema, introducing a breaking change can cause function calls to start failing or data extraction to produce unexpected errors.

### 6.2. Versioning Strategies

To manage this evolution, developers should adopt robust versioning strategies. The most common approach is to include version information directly in calls or in the schemas themselves.

**Versioning Patterns:**

1. **Versioning in URI (for APIs):** Include the version number in the API endpoint (e.g., `/api/v2/users`). While common, this is less directly applicable to LLM schemas.
2. **Versioning in Tool/Schema Name:** A practical approach is to incorporate the version in the tool or schema name. For example, `getUserProfile_v2` or `UserProfileV2`.
3. **Version Field in Schema:** Add a `"schema_version": "2.1.0"` field to the JSON schema itself. This allows processing code to handle different versions programmatically.

### 6.3. Compatibility and Migration

The concept of **schema compatibility**, popularized by tools like Confluent Schema Registry in the data streaming world, is directly applicable here. Compatibility rules define how schemas can evolve.

- **Backward Compatibility:** A new schema can read data produced with an old schema. This is achieved mainly by avoiding field removal and only adding optional fields.
- **Forward Compatibility:** An old schema can read data produced with a new schema. This requires all new fields added in the new schema to have default values.
- **Full Compatibility:** The schema is both backward and forward compatible.

For LLM systems, **backward compatibility** is most critical. Code that processes LLM output must be able to handle outputs generated based on older schemas that the model may have in its context. When making changes, best practices are:

1. **Add, don't remove:** Prefer adding new optional fields over removing or renaming existing ones.
2. **Use `deprecated`:** If a field needs to be removed, first mark it as deprecated in the schema and tool documentation. This signals to schema "users" (including the LLM and other developers) that the field will be removed in a future version.
3. **Plan migration:** Treat schema evolution like a database migration. Have a plan for updating client code, retraining or fine-tuning the model with new schemas, and monitoring validation failures that might indicate compatibility issues.

## 7. Practical Implementation Guide

This section provides a step-by-step guide for implementing function calling with concrete code examples.

### 7.1. Implementing Function Calling (OpenAI API Example)

**Step 1: Define Your Function in Code**

Create the Python function you want the LLM to be able to call:

```python
import json

def get_user_details(user_id: int):
    """Gets user details from their ID."""
    if user_id == 1:
        return json.dumps({"name": "Alice", "email": "alice@example.com"})
    return json.dumps({"error": "User not found"})
```

**Step 2: Create Tool Definition (JSON Schema)**

Describe the function in a format the LLM API understands:

```json
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_user_details",
            "description": "Gets user details from their ID.",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_id": {
                        "type": "integer",
                        "description": "The user's ID."
                    }
                },
                "required": ["user_id"]
            }
        }
    }
]
```

**Step 3: Call the LLM API with Tools**

Send the user query and tool definitions to the LLM:

```python
from openai import OpenAI
client = OpenAI()

messages = [{"role": "user", "content": "Can you give me the details for user with ID 1?"}]
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)
```

**Step 4: Analyze Response and Execute Function**

Check if the LLM decided to call a function. If so, execute it:

```python
response_message = response.choices[0].message
tool_calls = response_message.tool_calls

if tool_calls:
    # The LLM wants to call a function
    function_name = tool_calls[0].function.name
    function_args = json.loads(tool_calls[0].function.arguments)

    if function_name == "get_user_details":
        function_response = get_user_details(user_id=function_args.get("user_id"))
```

**Step 5: Send Result Back to LLM**

Add the function response to the conversation history and call the LLM again so it can formulate a final natural language response:

```python
messages.append(response_message)  # Add assistant's response (with tool call)
messages.append(
    {
        "tool_call_id": tool_calls[0].id,
        "role": "tool",
        "name": function_name,
        "content": function_response,
    }
)

final_response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=messages
)
print(final_response.choices[0].message.content)
# Expected output: "Sure, the details for user with ID 1 are: Name: Alice, Email: alice@example.com."
```

### 7.2. Practical Examples

**Example 1: Data Extraction with JSON Schema**

*Prompt:* `Extract the information from the following text and format it according to the provided JSON Schema. Text: "John Smith, email john.smith@email.com, is 30 years old." JSON Schema: {"type": "object", "properties": {"name": {"type": "string"}, "email": {"type": "string"}, "age": {"type": "integer"}}}`

*Expected Output:* `{"name": "John Smith", "email": "john.smith@email.com", "age": 30}`

**Example 2: Function Calling for a Calculator**

*User Query:* `What is 125 times 4?`

*LLM Decision:* Call the `calculator` function.

*Structured Output (Tool Call):* `{"name": "calculator", "arguments": {"expression": "125 * 4"}}`

*Tool Result:* `500`

*Final LLM Response:* `125 times 4 equals 500.`

### 7.3. Risk Mitigation Strategies

| Risk/Bias | Description | Mitigation |
| :--- | :--- | :--- |
| **Incorrect Function Calls** | The LLM may hallucinate arguments, call the wrong function, or call it when it shouldn't. | **1. Strict Validation:** Always validate received arguments before executing the function. Never blindly trust LLM input. **2. Clear Descriptions:** The clearer the function and parameter descriptions, the lower the chance of the LLM making mistakes. **3. Few-Shot Prompting:** Provide examples of conversations that led (or didn't lead) to a function call. |
| **Security (Code Injection)** | If a function's output is used to execute system commands (e.g., `os.system`), an attacker could create a prompt to inject malicious commands through function arguments. | **1. NEVER Execute Direct Commands:** Tools should be safe and isolated. They should interact with well-defined APIs, not the system shell. **2. Principle of Least Privilege:** The application executing functions should have minimum necessary permissions. **3. Sanitize All Inputs:** Treat arguments generated by the LLM as any other untrusted user input. |
| **Fragile JSON Parsing** | Before native JSON support, it was common to ask the LLM to generate JSON and then parse the text. This is fragile and can break if the model adds extra text. | **1. Use Native Modes:** Whenever possible, use native JSON output or function calling modes from the LLM API (e.g., `response_format={"type": "json_object"}` in OpenAI's API). They guarantee the output will be valid JSON. |
| **Tool Logic Complexity** | Managing multiple sequential or parallel tool calls can become complex. | **1. Agent Frameworks:** Use frameworks like LangChain, LlamaIndex, or OpenAI's Assistants API, which abstract much of the complexity of managing state, history, and tool execution logic. |

### 7.4. Quality Checklist

Before deploying a structured output or tool calling implementation, verify:

- [ ] **Structure Necessity:** Does the task benefit from structured output or is free-form text sufficient?
- [ ] **Native Mode Usage:** Is the native JSON or function calling mode from the API being used to guarantee output validity?
- [ ] **Well-Defined Schema:** Is the JSON Schema or function definition clear, complete, and with semantic descriptions?
- [ ] **Output Validation:** Is there a validation step in the code to ensure the LLM's output is correct before use?
- [ ] **Tool Security:** Are tools safe and do they not expose the system to code injection or other attack risks?
- [ ] **Error Handling:** Can the system gracefully handle malformed outputs or failed function calls?
- [ ] **Tool Atomicity:** Are tools focused on a single task?
- [ ] **LLM Feedback:** Is the tool execution result returned to the LLM so it can continue the conversation?
- [ ] **Evaluation Foundation:** Is structured output being used to enable automatic system evaluation?
- [ ] **Abstraction (if needed):** For complex logic, is an agent framework being considered to manage complexity?

## 8. Conclusion and Future Perspectives

The journey of structured outputs and tool calling in LLMs is one of increasing sophistication and power. What started as an attempt to force JSON syntax has evolved into a rich ecosystem of guaranteed schemas, flexible tools, semantic validation, and parallel orchestration. This evolution is the backbone of developing robust, reliable, production-ready agentic systems.

**Key Takeaways:**

1. **Schema Guarantee is the New Standard:** JSON Mode is obsolete for critical applications. Adherence to a JSON Schema, whether through function calling, tool use, or native API support, is now the industry standard for ensuring predictable outputs.

2. **Flexibility is Increasing:** The introduction of Custom Tools (free-form) and Grammar Constraints (CFG) by GPT-5 shows that the future isn't just about JSON. It's about giving the model the ability to generate the exact output an external system requires, whether it's a SQL string, a shell command, or a YAML configuration.

3. **Validation Must Be Semantic and Resilient:** Rule-based validation is necessary but insufficient. Semantic validation, which uses LLMs to evaluate content against subjective criteria, combined with self-healing recovery patterns, is essential for robustness in production.

4. **Orchestration is Becoming Parallel:** Sequential tool execution is a significant bottleneck. Patterns like Divide-Then-Aggregate (DTA), enabled by parallel tool calls, are defining the future of agent orchestration, leading to massive gains in latency and cost efficiency.

5. **Evolution Requires Discipline:** Managing schema evolution through careful versioning and backward compatibility practices is fundamental to the long-term maintenance of complex agentic systems.

The future points toward even deeper integration between LLMs and traditional software systems. We can expect to see more sophisticated tools for debugging tool calls, cost optimizers for agent orchestration, and the emergence of higher-level design patterns for building collaborative multi-agent systems. For developers, mastering the principles of structured outputs, robust validation, and efficient orchestration is no longer optional but a fundamental requirement for building truly transformative AI applications.

---

## References

[1] Agenta. (2025, September 10). *The guide to structured outputs and function calling with LLMs*. https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms

[2] Anthropic. (2025, November 14). *Structured outputs on the Claude Developer Platform*. https://www.claude.com/blog/structured-outputs-on-the-claude-developer-platform

[3] DataCamp. (2025). *GPT-5 Function Calling Tutorial*. https://www.datacamp.com/tutorial/gpt-5-function-calling-tutorial

[4] Cooper, A. (n.d.). *Constrained Decoding and Context-Free Grammars (CFG)*. https://www.aidancooper.co.uk/constrained-decoding/

[5] Liu, J. (2025, May 20). *Understanding Semantic Validation with Structured Outputs*. Instructor. https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/

[6] Zhu, D., et al. (2025). *Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation*. arXiv. https://arxiv.org/abs/2501.12432

[7] Schluntz, E., & Zhang, B. (2024, December 19). *Building effective agents*. Anthropic. https://www.anthropic.com/research/building-effective-agents

[8] OpenAI. (n.d.). *Parallel Function Calling*. OpenAI API Documentation. https://platform.openai.com/docs/guides/function-calling/parallel-function-calling

[9] Sturgeon, P. (n.d.). *API Versioning Has No "Right Way"*. Medium. https://medium.com/apis-you-wont-hate/api-versioning-has-no-right-way-f3c75457c0b7

[10] Confluent. (n.d.). *Schema Evolution and Compatibility for Schema Registry*. Confluent Documentation. https://docs.confluent.io/platform/current/schema-registry/fundamentals/schema-evolution.html

[11] OpenAI. (n.d.). *Function Calling*. OpenAI API Documentation. https://platform.openai.com/docs/guides/function-calling

[12] JSON Schema. (n.d.). *JSON Schema Documentation*. https://json-schema.org/

[13] LangChain. (n.d.). *Tools*. LangChain Documentation. https://python.langchain.com/docs/modules/tools/

[14] Vellum. (n.d.). *When should I use function calling, structured outputs or JSON mode*. Vellum Blog. https://www.vellum.ai/blog/when-should-i-use-function-calling-structured-outputs-or-json-mode

